{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bedbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "# 1. 初期設定\n",
    "START_URL = \"https://www.musashino-u.ac.jp/\"\n",
    "DOMAIN = urlparse(START_URL).netloc\n",
    "SLEEP_TIME = 0.5  # Webサーバーへの負荷軽減のため、待機時間を設定（秒）\n",
    "MAX_PAGES = 6000  # URLの上限\n",
    "# サイトマップ格納用の辞書\n",
    "sitemap = {}\n",
    "# 既に処理済みのURLを記録し、無限ループや重複アクセスを防ぐためのセット\n",
    "visited_urls = set()\n",
    "# 処理待ちのURLのリスト（キューとして利用）\n",
    "urls_to_visit = [START_URL]\n",
    "print(f\"--- スクレイピング開始: {START_URL} の同一ドメインを探索 ---\")\n",
    "# 静的ファイル拡張子の除外セット\n",
    "EXCLUDE_EXTS = {\n",
    "    '.pdf', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.bmp', '.ico', '.webp'\n",
    "}\n",
    "# 2. メインのスクレイピングループ\n",
    "while urls_to_visit and len(sitemap) < MAX_PAGES:\n",
    "    current_url = urls_to_visit.pop(0)\n",
    "    # 既に訪問済み、または同一ドメインでない場合はスキップ\n",
    "    if current_url in visited_urls or urlparse(current_url).netloc != DOMAIN:\n",
    "        continue\n",
    "    print(f\"アクセス中: {current_url}\")\n",
    "    visited_urls.add(current_url)\n",
    "    # 3. 負荷軽減のための待機\n",
    "    time.sleep(SLEEP_TIME)\n",
    "    try:\n",
    "        # 4. ページへのアクセス\n",
    "        response = requests.get(current_url, timeout=5)\n",
    "        response.raise_for_status()  # HTTPエラーがあれば例外を発生させる\n",
    "        response.encoding = response.apparent_encoding  # 文字化け対策\n",
    "        # 5. Beautiful Soupでパース（高速化のためlxmlが入っていれば推奨）\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # 6. <title>タグの抽出と辞書への格納\n",
    "        title_tag = soup.find('title')\n",
    "        page_title = title_tag.string.strip() if title_tag and title_tag.string else \"タイトルなし\"\n",
    "        sitemap[current_url] = page_title\n",
    "        # 7. 同一ドメインの新しいリンクを抽出\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            # 絶対URLに変換\n",
    "            absolute_url = urljoin(current_url, link['href'])\n",
    "            # クエリパラメータやフラグメントを削除してクリーンなURLにする\n",
    "            clean_url = absolute_url.split('#')[0].split('?')[0]\n",
    "            # 同一ドメイン以外はスキップ\n",
    "            if urlparse(clean_url).netloc != DOMAIN:\n",
    "                continue\n",
    "            # 静的ファイル拡張子の除外\n",
    "            path = urlparse(clean_url).path.lower()\n",
    "            if any(path.endswith(ext) for ext in EXCLUDE_EXTS):\n",
    "                continue\n",
    "            # 未訪問かつ未登録であれば、リストに追加\n",
    "            if clean_url not in visited_urls and clean_url not in urls_to_visit:\n",
    "                urls_to_visit.append(clean_url)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # アクセスエラーの処理\n",
    "        print(f\"エラー: {current_url} - {e}\")\n",
    "    except Exception as e:\n",
    "        # その他エラーの処理\n",
    "        print(f\"処理エラー: {current_url} - {e}\")\n",
    "# 8. 最終結果の表示 (print()で表示する要件)\n",
    "print(\"\\n--- サイトマップ結果 ---\")\n",
    "for url, title in sitemap.items():\n",
    "    print(f\"URL: {url}\")\n",
    "    print(f\"Title: {title}\")\n",
    "    print(\"-\" * 20)\n",
    "# 件数を表示\n",
    "print(f\"\\n--- 取得件数 ---\")\n",
    "print(f\"総URL数: {len(sitemap)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
